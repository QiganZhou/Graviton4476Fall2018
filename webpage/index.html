<!DOCTYPE html>
<html lang="en"><head>  
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <title>Computer Vision Class Project
  | CS, Virginia Tech | Fall 2018: CS 4476</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="author" content="">

<!-- Le styles -->  
  <link href="css/bootstrap.css" rel="stylesheet">
<style>
body {
padding-top: 60px; /* 60px to make the container go all the way to the bottom of the topbar */
}
.vis {
color: #3366CC;
}
.data {
color: #FF9900;
}
p {
	display: inline
}

</style>
  
<link href="css/bootstrap-responsive.min.css" rel="stylesheet">

<!-- HTML5 shim, for IE6-8 support of HTML5 elements --><!--[if lt IE 9]>
<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->
</head>

<body>
<div class="container">
<div class="page-header">

<!-- Title and Name --> 

<h1>Project Proposal <br> Finding the "Chirps" of Gravitational Waves</h1>
<img style="position: absolute; height: 70px; right:20px; top:60px; border:none;" src="GT_icon.png">
<span style="font-size: 20px; line-height: 1.5em"><strong>Xueying Huang, Daniel Mullen, Adele Yunlan Sun, Gonghan Xu, Zhigen Zhao, Qigan Zhou </strong></span><br>
<span style="font-size: 18px; line-height: 1.5em">Fall 2018 CS 4476 Computer Vision: Class Project</span><br>
<span style="font-size: 18px; line-height: 1.5em">Georgia Tech</span>
<hr>

<!-- problem statement -->
<h3>Problem Statement</h3>
<p style="font-size: 14px"> 
	On September 14, 2015, the Laser Interferometer Gravitational-Wave Observatory (LIGO) detected gravitational waves directly for the first time in human history. This will certainly be considered one of the most significant events in the 21st century. It marks the beginning of an era that humans can not only “see” the universe through electromagnetic waves but also “hear” the universe through gravitational waves! From now on, human beings are empowered by an extra sense.
	<br><br>
	The problem is that Einstein’s general relativity predicts that gravitational waves carry linear momentum, but we have no evidence of this effect yet. Fortunately, the amount of net linear momentum carried by the gravitational wave emitted by coalescing binary black holes can be inferred from the recoil velocity (or kick) of the merged final black hole. If we can observe such a kick, it would provide evidence that gravitational waves carry linear momentum. The existence of such a kick is closely related to the high-intensity, high-frequency features (called chirps) in the time-frequency maps of gravitational waves, as shown in <b>Figure 1</b>.
</p>

<!-- Figure 1 -->
<figure>
	<img src="figure1.png" style="height: 400px" class= "center">
	<figcaption style="text-align: center"> <b>Figure 1</b>: A sample time-frequency map of simulated gravitational wave. The chirps are marked out by red circles.</figcaption>
</figure>

<p style="font-size: 14px">
For practical reasons, we want to first study the chirp features on the gravitational waveforms produced by numerical relativity simulations of binary black holes. It will be beneficial to use computer vision algorithms to automatically process these time-frequency maps. <b>Therefore, the goal of our project is to apply techniques of computer vision to automatically label these time-frequency maps in terms of the chirp features</b>.<br><br>

Specifically, the <b>inputs</b> to our system are time-frequency maps of simulated gravitational waves at a specific time window around the merger time (defined at t=0 sec) of binary black holes. The time window will be roughly around (-0.05 sec, 0.05 sec) since physically we are only interested in chirps within this domain. The time-frequency map will be an intensity map, accompanied by a time array and a frequency array that define the coordinates of the two-dimensional intensity map. <b>Figure 1</b> shows such a sample input.<br><br>

The <b>output</b> of our system is closely related to three questions that we want to answer given a time-frequency map. First, how many chirps are there in the map? Second, where is each chirp on the time-frequency map? Third, what is the approximate relative total intensity (you can think of it as total pixel intensity) of each chirp? Therefore, given a time-frequency map as the input, the <b>output</b> of our system will be
<ol>
	<li>The number of chirps presented in this time-frequency map</li>
	<li>The times corresponding to each identified chirp</li>
	<li>The approximate total intensity of each chirp, in a relative sense. Physically, we care more about whether a chirp is relatively strong or weak in terms of its total intensity instead of the exact value of its total intensity.</li>
</ol>
Besides providing a system to calculate the outputs, we want to compare different approaches to this problem. The details will be elaborated upon in the experiments and results section.</p>
<br><br><br>


<!-- Approach -->
<h3>Approach</h3>
<p style="font-size: 14px">In this section, we will refer to the time-frequency maps as “images” so as to establish a closer connection to image processing.<br><br>

In order to achieve the desired results of counting and locating the chirps and characterizing the intensities of them, several computer vision techniques can be employed to process the time-frequency images.<br><br>

For example, to identify a region where enough chirp signal is present, the original time-frequency image can be thresholded into a binary image. The conventional thresholding methods are unlikely to perform well because the original image is not likely to be bimodal with clear separation. To overcome this problem, adaptive thresholding can be used to locally binarize the image.<br><br>

Since all chirps are approximately corners, Harris corner detector is useful in localizing the chirps in the binarized image. Subsequently, chirp selection can be done by analyzing the properties of each corner detected. Alternatively, chirps can be found by identifying connected components within the image above each row of pixels. The chirps are then located at the top most pixels of those connected components. The connected components can be found using either recursive search or the 2-pass labeling method as covered in lecture.<br><br>

If the traditional image processing methods mentioned above are not able to produce accurate chirp counting results, convolutional neural networks can be used. Since all images in the datasets are labelled (see the experiments and results section), it is feasible to train a neural network with the labelled training set and evaluate the performance of the algorithm with the test set.<br><br>

Once the chirps are counted and localized, the intensity information can be retrieved from the original time-frequency images by separating each identified chirp region from the base of the wave signal. This can potentially be done by locating the low-intensity (dark) corners between two adjacent chirps using the Harris corner detector. Another way is to pinpoint the frequency troughs of the boundary between the foreground and the background, where the boundary can be extracted with the Canny edge detector.
</p>
<br><br><br>

<!-- Experiments & Results -->
<h3>Experiments and Results</h3>
<p style="font-size: 14px">
	In the problem statement, we have given a general idea of how this project will proceed. In this section we provide more details.<br><br>

	<span style="font-size: 16px">Raw data and preprocessing</span><br>
	The inputs described in the problem statement, i.e. the time-frequency maps, need to be preprocessed from raw waveform data in the format of HDF5 files. These raw waveform data were produced by the numerical relativity group at Georgia Tech and are publicly available at <a href="http://www.einstein.gatech.edu/table/">http://www.einstein.gatech.edu/table/</a>.<br><br>

	<span style="font-size: 16px">The first preprocessing step</span><br>
	Given a waveform file and two orientation angles of the binary black holes, iota and phi, we can obtain a corresponding gravitational wave signal. We then need to perform a continuous wavelet transformation on this wave signal to obtain a starting time-frequency map of a relatively broad time-frequency spectrum. The frequency spectrum ranges from about 0 Hz to 500 Hz as integers, while the time spectrum depends on the specific input waveform file and the orientation angles but will always encompass t=0 sec. Since one of our team members is currently doing research in the numerical relativity group at Georgia Tech, we will use existing python scripts from the numerical relativity group to transform an HDF5 waveform file and two orientation angles into the starting time-frequency map. These existing scripts are available publicly at <a href="https://github.com/astroclark/bbh-tfmaps">https://github.com/astroclark/bbh-tfmaps/</a>.<br><br>

	<span style="font-size: 16px">The second preprocessing step</span><br>
	We will then need to further preprocess the starting time-frequency map to narrow it down to a smaller time window around (-0.05 sec, 0.05 sec). We might also potentially downsample the pixels of the starting time-frequency map to a lower (but good enough) resolution so that we can deal with smaller images. After this, we can obtain the input time-frequency maps of the desired format as described in the problem statement.<br><br>

	<span style="font-size: 16px">Data sets</span><br>
	Although our fundamental approaches to this problem are the signal processing techniques that we learned in class, we will still borrow the concepts of training and testing data from machine learning. We are going to prepare two separate sets of data -- one for developing our system (“training”) and the other for testing. Each instance of the training and test sets consists of a 2-D intensity array, a time array, and a frequency array. Each instance will be labelled manually in terms of
	<ol>
		<li>The number of chirps presented in this time-frequency map</li>
		<li>The time corresponding to each identified chirp</li>
	</ol>
	Notice that we will not label the instances in terms of the relative intensity of each chirp because labelling the intensity by eyeballing is subject to significant errors.<br><br>

	We plan to prepare around 100 training instances and around 100 testing instances, which we deem sufficient for developing and testing our system in the scope of this problem. Besides, we will not use all the available waveforms from the Georgia Tech waveform repository, since many of the waveform files will produce overall similar time-frequency maps and it is often the orientation angles (iota and phi) that vary the appearances of the time-frequency maps. So, to maximize the representativeness of the training and testing sets, we will manually select a diversity of time-frequency maps in terms of the chirp feature rather than the number of base waveform files.<br><br>


	<span style="font-size: 16px">Data collection protocol</span><br>
	Summarizing what we have described in the previous sections, we now give a enumeration of the steps that we will use to build our data sets.
	<ol>
		<li>The raw input data are the public numerical relativity gravitational waveform files in HDF5 format, collected from Georgia Tech’s public waveform repository at <a href="http://www.einstein.gatech.edu/table/">http://www.einstein.gatech.edu/table/</a>.</li>
		<li>These raw input data, along with suitable orientation angles, will be preprocessed into time-frequency maps, the desired input data. There are two preprocessing steps. In the first step we will use existing preprocessing programs from the numerical relativity group at Georgia Tech, and in the second step we will implement our own preprocessing algorithm.</li>
		<li>Each (preprocessed) input time-frequency map will be manually labeled. The two image-level labels are (1) the number of chirps present (2) the time position of each chirp. These input time-frequency maps will be separated into training and testing sets, with roughly 100 examples in each.</li>
	</ol>
	Though the raw data and the first-step preprocessing code are provisioned by the numerical relativity group at Georgia Tech, our team will play the most essential part in the data collection process.

	Notice that since the input time-frequency maps come from numerical relativity simulations and their appearances will be manually inspected before they are used as training and testing instances, for the purpose of this project they can be regarded as error-free. Therefore, the major source of error of the data sets will come from manually labelling the time-frequency maps. Note that some chirps on a time-frequency map can be very faint in intensity so that they might be missed in the labelling process. Moreover, the time positions of the identified chirps can only be manually labelled to an approximate extent, so we will always need to allow some tolerance when comparing the time-position labels to the outputs of our system.<br><br>


	<span style="font-size: 16px">Implementation</span><br>
	Apart from the aforementioned second step of preprocessing, we will implement algorithms to automatically label an input time-frequency map and visualize the results. To avoid confusion with the manual labelling of the data sets, we will call the automatic labelling process of our system “<b>marking</b>”. In the marking process, we plan to implement two different approaches for counting the number of chirps and locating their time positions and one approach for calculating the relative total intensity of each chirp. We will also write programs to compare and visualize the performance of the different approaches and the overall performance our system against the labels of the data sets.<br><br>


	<span style="font-size: 16px">Experiments</span><br>
	There are three main experiments of our project.
	<ol>
		<li>Develop an algorithm to identify each chirp in a time-frequency map and count the total number of chirps.</li>
		<li>Develop a different algorithm to identify each chirp in a time-frequency map and count the total number of chirps.</li>
		<li>Develop an algorithm to calculate the relative total intensity of each chirp. This algorithm may be based on the outputs of the previous two algorithms.</li>
	</ol>
	All three algorithms are to be developed only using the training set and then evaluated against the test set. At this stage, we are uncertain about
	<ol>
		<li>How well our system’s marking will be able to match the labels in the training set</li>
		<li>How well our two different approaches to identifying and counting the chirps will agree</li>
		<li>How well our system will generalize to the test set.</li>
	</ol>	
	Closely linked to the aforementioned uncertainties, our expectation is that our system will mark the time-frequency maps in good accordance with the labels in both the training set and the test set. We also expect that our different approaches to the same problem will give close and accurate results.<br><br>


	<span style="font-size: 16px">Evaluation metric</span><br>
	At this stage, given the unavoidable errors in our labelled data sets and the uncertainties of the experimental outcomes, the success of this project is defined below:
	<ol>
		<li>For counting the number of chirps, an accuracy of over 70% for at least one algorithm (out of the two) over the entire test set. Specifically, we want (the number of test instances whose labelled number of chirps match our system’s calculation / the total number of test instances) > 70%. We think 70% is a reasonable criterion because some chirps with low intensities can be very ambiguous and there are a lot of low intensity chirps in our data sets.</li>
		<li>For identifying the time locations of the chirps, a total accuracy of over 65% for the same algorithm (the one that passes the first criterion) over the entire test set. Specifically, we want (the total number of correctly auto-marked, pre-labelled chirps in the test set / the total number of pre-labelled chirps in the test set) > 65%. We define a tolerance window of 0.01 sec such that if the time position of a chirp marked by our system deviates from the corresponding labelled time position by less than 0.01 sec, then we consider it as a correct marking. We define such a tolerance window and a slightly looser metric for success in this criterion than the first one because in addition to the errors involved in identifying the ambiguous chirps, the manual labelling of the time positions are likely to include nontrivial error.</li>
		<li>For identifying the relative intensities of chirps, instead of using a quantitative metric, we will only evaluate it qualitatively by manually examining the outputs of our system over the entire test set. If we find the output over a test instance generally reasonable, we will then mark the test result of this instance as accurate. We want to achieve an accuracy rate of above 70% over the entire test set, due to the same reason for the first criterion.</li>
	</ol>
</p>
<br>


<h3>Team Members</h3>
<ul style="font-size: 12pt">
	<li>Xueying Huang: xhuang327</li>
	<li>Daniel Mullen: dmullen3</li>
	<li>Adele Yunlan Sun: ysun379</li>
	<li>Gonghan Xu: gxu36</li>
	<li>Zhigen Zhao: zzhao300</li>
	<li>Qigan Zhou: qzhou73</li>
</ul>

  <hr>
  <footer>
  <p style="font-size: 14px">© Xueying Huang, Daniel Mullen, Adele Yunlan Sun, Gonghan Xu, Zhigen Zhao, Qigan Zhou</p>
  </footer>
</div>
</div>

<br><br>

</body></html>
